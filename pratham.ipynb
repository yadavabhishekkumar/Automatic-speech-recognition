{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Algorithmia\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "import pandas as pd\n",
    "import re\n",
    "from summa import commons, graph, keywords, pagerank_weighted\n",
    "from summa import summarizer, syntactic_unit, textrank\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"brown bag/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"pratham/content.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['story_id', 'story_title', 'story_english_title',\n",
       "       'is_child_created_story', 'stories_status', 'stories_summary',\n",
       "       'ancestry', 'is_recommended_story', 'reads', 'language_name',\n",
       "       'organization_name', 'page_type', 'story_derivation_type',\n",
       "       'story_publishing_type', 'reading_level_cat', 'story_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishekkumaryadav\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\abhishekkumaryadav\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_text['story_content'] = data.loc[:,'story_content']\n",
    "data_text['index'] = data.loc[:,'story_id']\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishekkumaryadav\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhishekkumaryadav\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "['A', 'fawn', 'was', 'racing', 'in', 'the', 'forest.', 'He', 'was', 'ahead', 'of', 'the', 'rabbit.', 'He', 'was', 'ahead', 'of', 'the', 'elephant.', 'He', 'leapt', 'and', 'cleared', 'the', 'stream.', 'He', 'ran', 'past', 'the', 'crumbling', 'wall.', 'There', 'was', 'a', 'large', 'boulder', 'on', 'the', 'grassy', 'plain.', 'He', 'stumbled', 'and', 'fell', 'down.', 'He', 'burst', 'into', 'tears.', 'The', 'monkey', 'massaged', 'his', 'leg.', 'Tears', 'flowed', 'from', 'the', \"fawn's\", 'eyes.', 'Brother', 'Bear', 'picked', 'him', 'up.', 'The', 'fawn', \"didn't\", 'stop', 'crying.', 'His', 'mother', 'came.', 'She', 'said,', '“Look,', 'we’ll', 'beat', 'up', 'this', 'bad', 'boulder!”', 'The', 'fawn', 'said,', '“Oh,', 'don’t', 'do', 'that', 'or', 'he', 'will', 'also', 'start', 'crying.”', 'His', 'mother', 'laughed.', 'So', 'did', 'the', 'fawn.']\n"
     ]
    }
   ],
   "source": [
    "#without preprocessing \n",
    "words=[]\n",
    "# list(documents[0,'story_content'])\n",
    "\n",
    "# w=\"i am superman\"\n",
    "w=documents.loc[0,'story_content']\n",
    "print(type(w))\n",
    "for word in w.split(' '):\n",
    "    words.append(word)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fawn', 'race', 'forest', 'ahead', 'rabbit', 'ahead', 'eleph', 'leap', 'clear', 'stream', 'past', 'crumbl', 'wall', 'larg', 'boulder', 'grassi', 'plain', 'stumbl', 'fell', 'burst', 'tear', 'monkey', 'massag', 'tear', 'flow', 'fawn', 'eye', 'brother', 'bear', 'pick', 'fawn', 'stop', 'cri', 'mother', 'come', 'say', 'look', 'beat', 'boulder', 'fawn', 'say', 'start', 'cri', 'mother', 'laugh', 'fawn']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying preprocess method to entire story_content\n",
    "processed_docs = list(map(preprocess,documents.loc[0:1300,'story_content']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)\n",
    "#processed_docs,processed_docs22,processed_docs13,processed_docs15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-303a9af8177d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1320\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprocessed_docs20\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'story_content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "processed_docs20=[]\n",
    "for i in range(1300,1320,1):\n",
    "    print(i)\n",
    "    processed_docs20.append(list(map(preprocess,documents.loc[i,'story_content'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n"
     ]
    }
   ],
   "source": [
    "processed_docs22=[]\n",
    "for i in range(1312,1320,1):\n",
    "    print(i)\n",
    "    processed_docs20.append(list(map(preprocess,documents.loc[i,'story_content'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs13 = list(map(preprocess,documents.loc[1320:1500,'story_content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs15 = list(map(preprocess,documents.loc[1500:2050,'story_content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_docs,processed_docs22,processed_docs13,processed_docs15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos function return pos tagging for a list of words  and returns a list\n",
    "def pos(lis):\n",
    "    return nltk.pos_tag(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_docs is a list with each element for each story and each element being a list of keywords\n",
    "processed_docsPOS=list(map(pos,processed_docs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docsPOS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagl=processed_docsPOS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagl[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating noun and adjective pos tags from this list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep(lis):\n",
    "    ans=[]\n",
    "    for i in lis:\n",
    "        if(i[1] in ['NN','NNS','NNP','NNPS','JJ','JJR','JJS']):\n",
    "            ans.append(i[0])\n",
    "    return ans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docsPOSSep=list(map(sep,processed_docsPOS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docsPOSSep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fawn',\n",
       " 'race',\n",
       " 'forest',\n",
       " 'rabbit',\n",
       " 'eleph',\n",
       " 'leap',\n",
       " 'clear',\n",
       " 'stream',\n",
       " 'crumbl',\n",
       " 'wall',\n",
       " 'larg',\n",
       " 'boulder',\n",
       " 'grassi',\n",
       " 'plain',\n",
       " 'stumbl',\n",
       " 'tear',\n",
       " 'monkey',\n",
       " 'massag',\n",
       " 'flow',\n",
       " 'fawn',\n",
       " 'eye',\n",
       " 'brother',\n",
       " 'bear',\n",
       " 'pick',\n",
       " 'fawn',\n",
       " 'cri',\n",
       " 'mother',\n",
       " 'beat',\n",
       " 'boulder',\n",
       " 'fawn',\n",
       " 'laugh',\n",
       " 'fawn']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docsPOSSep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec  gives a vector for a document\n",
    "#word2vec gives a vector for a word\n",
    "#calculating the similarity score b/w word and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(processed_docsPOSSep, min_count=1,size=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10882, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['fawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.42139171e-02, -4.21236232e-02, -9.35103297e-02,  5.01881987e-02,\n",
       "       -2.57723331e-02, -9.10245478e-02,  1.64841533e-01,  1.10006794e-01,\n",
       "        2.19817638e-01, -7.09779561e-02,  1.25355020e-01, -1.52739584e-01,\n",
       "        7.71980956e-02, -1.07686646e-01, -4.22223285e-02,  1.01133771e-01,\n",
       "        7.81253353e-03,  1.08543999e-01, -1.89336352e-02, -2.69440152e-02,\n",
       "       -2.14953080e-01,  1.24336176e-01, -3.32065448e-02,  8.62144530e-02,\n",
       "       -6.46366831e-03,  1.18876761e-03, -8.06769729e-02, -1.66203424e-01,\n",
       "       -2.09112875e-02, -6.37922883e-02, -1.17638983e-01, -3.63181718e-03,\n",
       "       -1.07466310e-01,  2.93545350e-02, -1.22062884e-01,  1.64637193e-01,\n",
       "        1.25240311e-01, -7.29128392e-03, -1.12215867e-02,  5.22533357e-02,\n",
       "        1.16484975e-02,  1.65237904e-01,  8.36973358e-03, -8.31604600e-02,\n",
       "       -1.65455621e-02, -5.28947562e-02,  8.27197134e-02,  2.79639006e-01,\n",
       "       -3.45793106e-02, -1.00270674e-01, -2.01229285e-02, -1.05815865e-01,\n",
       "       -3.85126621e-02, -2.65231449e-02,  1.96314460e-04, -4.00435552e-02,\n",
       "        9.59743839e-03,  2.76459809e-02, -4.60259765e-02, -9.01558530e-03,\n",
       "        1.10428512e-01,  3.11184395e-02, -6.20231852e-02,  2.51534116e-02,\n",
       "        2.97101997e-02,  4.14785594e-02, -1.77291900e-01,  1.22266114e-01,\n",
       "       -6.82316422e-02, -9.38115194e-02, -2.56529246e-02,  1.81617588e-01,\n",
       "       -1.54962018e-01,  2.73077302e-02, -1.70726061e-01,  9.14621577e-02,\n",
       "       -3.13797444e-02, -9.84187052e-02,  5.98262027e-02,  5.67109995e-02,\n",
       "       -5.28074093e-02, -1.21035419e-01,  2.62607429e-02,  1.05592748e-02,\n",
       "        1.37666520e-02, -2.19185486e-01,  2.01259196e-01, -1.12709984e-01,\n",
       "        8.05888698e-02, -1.84630491e-02,  6.35377839e-02, -1.69736043e-01,\n",
       "        1.42833129e-01, -1.32897720e-01,  3.88732441e-02,  4.50163931e-02,\n",
       "        7.62818754e-02,  4.97025289e-02, -4.70045023e-02, -9.31691155e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docsPOSSep)]\n",
    "model1 = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['fawn', 'race', 'forest', 'rabbit', 'eleph', 'leap', 'clear', 'stream', 'crumbl', 'wall', 'larg', 'boulder', 'grassi', 'plain', 'stumbl', 'tear', 'monkey', 'massag', 'flow', 'fawn', 'eye', 'brother', 'bear', 'pick', 'fawn', 'cri', 'mother', 'beat', 'boulder', 'fawn', 'laugh', 'fawn'], tags=[0])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(common_texts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model1.infer_vector(processed_docsPOSSep[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10592639, -0.01341358, -0.9744336 ,  0.53871536,  0.02754767],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document vector size is currently 5 and word vector size is 100\n",
    "#will have to make equal for both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will apply MMD equation  S=λA- (1-λ) max sim(Wi,Wj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fixed λ\n",
    "# to 0.5 in the adapted MMR equation (2), to ensure\n",
    "# equal importance to informativeness and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (100,) not aligned: 5 (dim 0) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-98b847799fd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#vector for this word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mvectorW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvectorW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mmaxSim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,) and (100,) not aligned: 5 (dim 0) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "#model1 is model for doc2vec\n",
    "#model is model for word2vec\n",
    "ST=[]\n",
    "for i in processed_docsPOSSep:\n",
    "    #vector for this document\n",
    "    vectorD=model1.infer_vector(i)\n",
    "    S=[]\n",
    "    for j in i:\n",
    "        #vector for this word\n",
    "        vectorW=model.wv[j]\n",
    "        A=numpy.dot(vectorD,vectorW)\n",
    "        maxSim=0\n",
    "        for k in i:\n",
    "            if(numpy.dot(model.wv[j],model.wv[k])>maxSim):\n",
    "                maxSim=numpy.dot(model.wv[j],model.wv[k])\n",
    "        S.append(0.5*A-0.5*maxSim)\n",
    "    ST.append(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summa keywords.keywords are not performing well as evident from the first story content output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fawn was racing in the forest He was ahead of the rabbit He was ahead of the elephant He leapt and cleared the stream He ran past the crumbling wall There was a large boulder on the grassy plain He stumbled and fell down He burst into tears The monkey massaged his leg Tears flowed from the fawns eyes Brother Bear picked him up The fawn didnt stop crying His mother came She said Look well beat up this bad boulder The fawn said Oh dont do that or he will also start crying His mother laughed So did the fawn\n",
      "fawn\n",
      "fawns eyes\n",
      "didnt\n",
      "oh dont\n",
      "bear\n",
      "['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']\n"
     ]
    }
   ],
   "source": [
    "a1=remove_special_characters(w)\n",
    "print(a1)\n",
    "a=keywords.keywords(a1)\n",
    "print(a)\n",
    "y=[s.strip() for s in a.splitlines()]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[\"story_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df[\"story_content\"][1314])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    \n",
    "    #checking that should we remove the digits as well or not\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    \n",
    "    #find the pattern in text and replace it with ''\n",
    "    text = re.sub(pattern,'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[]\n",
    "for item in range(len(x)):\n",
    "    if(item == 1314):\n",
    "        continue\n",
    "    a1=remove_special_characters(x[item])\n",
    "    a=keywords.keywords(a1)\n",
    "    y=[s.strip() for s in a.splitlines()]\n",
    "    x1.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keyword(x,i,f):\n",
    "    k=[]\n",
    "    for item in x[i:f]:\n",
    "        print(\"##########1\")\n",
    "        a1=remove_special_characters(item)\n",
    "        print(a1)\n",
    "        a=keywords.keywords(a1)\n",
    "        print(\"##########2\")\n",
    "        print(a)\n",
    "        y=[s.strip() for s in a.splitlines()]\n",
    "        print(\"##########3\")\n",
    "        print(y)\n",
    "        k.append(y)\n",
    "        print(\"##########4\")\n",
    "        print(k)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########1\n",
      "A fawn was racing in the forest He was ahead of the rabbit He was ahead of the elephant He leapt and cleared the stream He ran past the crumbling wall There was a large boulder on the grassy plain He stumbled and fell down He burst into tears The monkey massaged his leg Tears flowed from the fawns eyes Brother Bear picked him up The fawn didnt stop crying His mother came She said Look well beat up this bad boulder The fawn said Oh dont do that or he will also start crying His mother laughed So did the fawn\n",
      "##########2\n",
      "fawn\n",
      "fawns eyes\n",
      "didnt\n",
      "oh dont\n",
      "bear\n",
      "##########3\n",
      "['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']\n",
      "##########4\n",
      "[['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']]\n"
     ]
    }
   ],
   "source": [
    "#since this didn't work\n",
    "# from summa import commons, graph, keywords, pagerank_weighted\n",
    "# from summa import summarizer, syntactic_unit, textrank\n",
    "\n",
    "a=add_keyword(x,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we tried this\n",
    "x1=[]\n",
    "for item in x[0:1000]:\n",
    "    a1=remove_special_characters(item)\n",
    "    a=keywords.keywords(a1)\n",
    "    y=[s.strip() for s in a.splitlines()]\n",
    "    x1.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it's definitely between 1300-1350\n",
    "#1400-1500 is safe\n",
    "#1500-1700 is safe\n",
    "#1700 to 1900 is safe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
